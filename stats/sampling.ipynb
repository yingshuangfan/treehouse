{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e4d5bdd",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "\n",
    "## Direct Sampling\n",
    "\n",
    "## Likelihood(weighted) Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d0d1f6",
   "metadata": {},
   "source": [
    "## Rejection Sampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae10114",
   "metadata": {},
   "source": [
    "## Importance Sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b6d91b",
   "metadata": {},
   "source": [
    "## Markov Chain Monte Carlo(MCMC)\n",
    "\n",
    "**Def. Metropolis-Hasting Algorithm:** Given target function $f(X)$(e.g. posterior), candidate function $q(x, y)$(e.g. joint posterior) or $q(y|x)$(e.g. full conditional), the procedure of drawing data samples is:\n",
    "1. Iinitialize $x^{(0)}$.\n",
    "2. At the k-th iteration, simulate $y \\sim q(x^{(k-1)}, y)$.\n",
    "3. Calcuate the current acceptance rate:\n",
    "\\begin{align*}\n",
    "\\alpha(x^{(k-1)}, y) = min\\left\\{ \\frac{f(y)q(y, x^{(k-1)})}{f(x^{(k-1)}) q(x^{(k-1)}, y)}, 1 \\right\\}\n",
    "\\end{align*}\n",
    "4. Simulate $u \\sim Unif(0, 1)$. Update $x$ as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "x^{(k)} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      y & u \\le \\alpha(x^{(k-1)}, y) \\\\\n",
    "      x^{(k-1)} & otherwise \\\\\n",
    "\\end{array} \n",
    "\\right. \n",
    "\\end{align*}\n",
    "\n",
    "**Corollary. Random Walk Metropolis:** The special case of Metropolis-Hasting Algorithm where $q(x, y) = q(y, x)$. \n",
    " - The probability of moving from x to y and y to x are identical. Thus we have the property of symmetric, and the acceptence rate could be simplified to:\n",
    " \\begin{align*}\n",
    "\\alpha(x^{(k-1)}, y) = min\\left\\{ \\frac{f(y)}{f(x^{(k-1)})}, 1 \\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "**Corollary. Independent Metropolis:** The special case of Metropolis-Hasting Algorithm where $q(x, y) = p(y)$.\n",
    " - The probability of moving from x to y is indepentent to x, in other words the probability of transfering to a certain state is independent to its previous states. The acceptence rate could be simplified to:\n",
    "\\begin{align*}\n",
    "\\alpha(x^{(k-1)}, y) = min\\left\\{ \\frac{f(y)p(x^{(k-1)})}{f(x^{(k-1)})p(y)}, 1 \\right\\}\n",
    "\\end{align*}\n",
    "\n",
    "### Gibbs Sampler\n",
    "\n",
    "**Def. Gibbs Sampler:** Draw data samples from the joint distribution $p(\\theta_1,\\theta_2,...,\\theta_m)$ by sampling $\\theta_i$ iteratively from the full conditional distribution $p(\\theta_i|\\theta_{-i}) = p(\\theta_i|\\theta_1,\\theta_2,...,\\theta_{i-1},\\theta_{i+1},...,\\theta_m)$, with the procedure as follows:\n",
    "1. Initialize $\\theta_1^{(0)},\\theta_2^{(0)},...,\\theta_m^{(0)}$.\n",
    "2. At the k-th iteration, for $i = 1,2,...m$, sample $\\theta_i$ from:\n",
    "\\begin{align*}\n",
    "\\theta_i^{(k)} \\sim p(\\theta_i|\\theta_1^{(k)},\\theta_2^{(k)},...,\\theta_{i-1}^{(k)},\\theta_{i+1}^{(k-1)},...,\\theta_m^{(k-1)})\n",
    "\\end{align*}\n",
    "\n",
    "Comments:\n",
    " - Gibbs sampler is a special case of Metropolis-Hasting Algorithm where the acceptence rate is always 1. Proof: let $x=(\\theta_i,\\theta_{-i})$ and $y=(\\theta_i^*,\\theta_{-i})$, the transition probability $q(x,y)=p(\\theta_i^*|\\theta_{-i})$(e.g. full conditional), thus the acceptence rate can be derived as:\n",
    "\\begin{align*}\n",
    "\\alpha(x, y) &= min\\left\\{ \\frac{f(y)q(y, x)}{f(x) q(x, y)}, 1 \\right\\} \\\\\n",
    "&= min\\left\\{ \\frac{p(\\theta_i^*,\\theta_{-i})p(\\theta_i|\\theta_{-i})}{p(\\theta_i,\\theta_{-i}) p(\\theta_i^*|\\theta_{-i})}, 1 \\right\\} \\\\\n",
    "&= min\\left\\{ \\frac{p(\\theta_i^*,\\theta_{-i})p(\\theta_i,\\theta_{-i})p(\\theta_{-i})}{p(\\theta_i,\\theta_{-i}) p(\\theta_i^*,\\theta_{-i})p(\\theta_{-i})}, 1 \\right\\} \\\\\n",
    "&= 1\n",
    "\\end{align*} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3b2dfc",
   "metadata": {},
   "source": [
    "## Monte Carlo Estimation (Application of Sampling)\n",
    "\n",
    "Intuition: Estimatation based on random sampling, a.k.a Monte Carlo Method. The methodology includes various implementations, a common adoption in statistics is estimating the integration(e.g. marginal probability) with complicated closed-form expression.\n",
    "\n",
    "**Def. Monte-Carlo Estimation:** The approximate expectation of $f(X)$, where random variable X follows distribution $P$, is the average of i.i.d sampled data $z_1,z_2,...,z_N$. \n",
    "\n",
    "\\begin{align*}\n",
    "E_{x \\sim p(x)}[f(x)] = \\int p(x)f(x) \\,dx \\approx \\frac{1}{N} \\sum_{i=1}^N f(z_i)\n",
    "\\end{align*}\n",
    "\n",
    "**Corollary. Estimation of Posterior Predictive with Monte Carlo Method:** The posterior predictive (integration of posterior over possible parameters $\\theta$) can be estimated by:\n",
    "\n",
    "\\begin{align*}\n",
    "P(x_{n+1}|D) &=\\int_{\\theta}{P(x_{n+1}|\\theta)P(\\theta|D)\\,d\\theta} \\\\\n",
    "& \\approx \\frac{1}{N} \\sum_{i=1}^N P(x_{n+1}|\\theta_i) \\\\\n",
    "& \\theta_i \\sim P(\\theta|D)\n",
    "\\end{align*}\n",
    "\n",
    "The estimation can be described as: \n",
    "1. Draw a sampled parameter $\\theta_i$ from the posterior $P(\\theta|D)$ with a sampler(e.g. Gibbs Sampler). \n",
    "2. Given the $\\theta_i$ in last step, draw a sampled new data $z_i$ from the likelihood $P(x_{n+1}|\\theta_i)$.\n",
    "3. Repeat step 1&2 in order to generate the i.i.d sampled data $z_1,z_2,...,z_N$, giving the size of such dataset(N) is sufficiently large, then the average of sampled data is an Monte Carlo Estimation of new data point $x_{new}$.\n",
    "\n",
    "Comments: \n",
    " - Notice that we only sample exactly ONE $z_i$ from any given $\\theta_i$ sampled from the posterior. The point here is that the size of $z_i$ doesn't matter, as long as we have sufficent samples of $\\theta_i$. In other words, the estimation here relies on taking the \"average\" over $\\,d\\theta$. For instance, a sampled data of 10k with 5k $\\theta$ is much more reliable than a sampled data of 100k with 500 $\\theta$.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
