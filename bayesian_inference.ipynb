{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e4d5bdd",
   "metadata": {},
   "source": [
    "# Bayesian Inference\n",
    "\n",
    "## Posterior\n",
    "\n",
    "**Def.1 Posterior Distribution:** The posterior distribution of parameter $\\theta$, based on observations $D=\\{x_1,...,x_n\\}$, likelihood function $L(\\theta|D)=P(D|\\theta)=\\prod_{i=1}^n{P(x_i|\\theta)}$(assuming data samples are i.i.d), prior distribution $\\pi(\\theta)$, is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\pi(\\theta|D)=\\frac{P(D|\\theta)P(\\theta)}{P(D)}=\\frac{P(D|\\theta)\\pi(\\theta)}{\\int_{\\theta}{P(D|\\theta)\\pi(\\theta)}}\n",
    "\\end{align}\n",
    "\n",
    " - To obtain the exact distribution(not the exact pdf) for $P(\\theta|D)$, we could ignore the marginal probability $P(D)$ as constant(regarding $\\theta$). Thus $P(\\theta|D) \\propto P(D|\\theta)P(\\theta)$. However, this case holds only if the form of such posterior resembles some well-known distributions, where we could find the kernel without considering the value of $P(D)$.\n",
    " \n",
    "**Corollary.1-1 Joint Posterior:** The posterior in Def.1 can be applied to multiple parameters $\\theta_1,...\\theta_m$, if given the joint prior:\n",
    "\n",
    "\\begin{align}\n",
    "\\pi(\\theta_1,...\\theta_m|D)=\\frac{P(D|\\theta_1,...\\theta_m)\\pi(\\theta_1,...\\theta_m)}{P(D)}\n",
    "\\end{align}\n",
    "\n",
    "**Corollary.1-2 Full Conditional Posterior:** For multiple parameters $\\theta_1,...\\theta_m$, the conditional posterior for i-th parameter $\\theta_i$, if given values $\\theta_{-i}$ for other parameters and data samples, is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\pi(\\theta_i|D,\\theta_{-i})&=P(\\theta_i|D,\\theta_1,...,\\theta_{i-1},\\theta_{i+1},...\\theta_m) \\\\\n",
    "&=\\frac{\\pi(\\theta_1,...\\theta_m|D)}{P(\\theta_1,...,\\theta_{i-1},\\theta_{i+1},...\\theta_m|D)} \\\\\n",
    "&=\\frac{\\pi(\\theta_1,...\\theta_m|D)}{\\int_{\\theta_i}{\\pi(\\theta_1,...\\theta_m|D)\\,d\\theta_i}}\n",
    "\\end{align}\n",
    "\n",
    " - The full conditional posterior relies on computing the joint posterior first.\n",
    " - In common practice, it is extremely complicated to compute the integral for each parameter $\\theta_i$. One possible approach is to **derive the full conditional posteroir directly from the joint posterior**, by ignoring the other parameters as constants. Thus $P(\\theta_i|D,\\theta_{-i}) \\propto P(\\theta_1,...\\theta_m|D)|_{\\theta_{-i}}$ However, this case holds only for well-known distributions.\n",
    " - The concept of full conditional is very important for Gibbs sampling method.\n",
    " \n",
    "**Corollary.1-3 Marginal Posterior:** For multiple parameters $\\theta_1,...\\theta_m$, the marginal posterior for i-th parameter $\\theta_i$, if given values $\\theta_{-i}$ for other parameters and data samples, is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\pi(\\theta_i|D)&=\\int_{\\theta_{-i}}{\\pi(\\theta_1,...\\theta_m|D)\\,d\\theta_{-i}} \\\\\n",
    "&=\\int_{\\theta_{-i}}{\\pi(\\theta_i,\\theta_{-i}|D)\\,d\\theta_{-i}} \\\\\n",
    "&=\\int_{\\theta_{-i}}{\\pi(\\theta_i|D,\\theta_{-i})P(\\theta_{-i}|D)\\,d\\theta_{-i}} \\\\\n",
    "\\end{align}\n",
    "\n",
    " - The marginal posterior relies on computing the joint posterior first, or the full conditional posterior first.\n",
    " - To obtain the exact distribution of marginal posterior, the integration might be simplified by ignoring some constants. Be aware that all parameters other than $\\theta_i$ should be integrated out. However, the case holds only for well-known distributions.\n",
    "\n",
    "## Posterior Predictive\n",
    "\n",
    "**Def.2 Posterior Predictive Distribution:** The posterior predictive distribution of parameter $\\theta$ is defined as(based on conditions given in Def.1):\n",
    "\n",
    "\\begin{align}\n",
    "P(x_{n+1}|D)=\\int_{\\theta}{P(x_{n+1},\\theta|D)\\,d\\theta}=\\int_{\\theta}{P(x_{n+1}|D,\\theta)P(\\theta|D)\\,d\\theta}=\\int_{\\theta}{P(x_{n+1}|\\theta)P(\\theta|D)\\,d\\theta}\n",
    "\\end{align}\n",
    "\n",
    " - The posterior preditive relies on computing the posterior first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81361d69",
   "metadata": {},
   "source": [
    "## Bayesian Prospective of MLP\n",
    "\n",
    "In general, a machine learning problem could also be viewed from the bayesian inference prospective. The relationship could be described as:\n",
    "\n",
    "| Bayesian Inference      | Machine Learning |\n",
    "| :-----------: | :-----------: |\n",
    "| Likelihood Function      | Loss Function       |\n",
    "| Prior Distribution   | Regularization        |\n",
    "| Posterior Distribution   | Regularized Loss Function       |\n",
    "| Maximize a Posterior Estimation(MAP)   | Minimize Regularized Loss Function       |\n",
    "| Maximize Likelihood Estimation(MLE)   | Minimize Loss Function       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57aeb97",
   "metadata": {},
   "source": [
    "## Laplace Approximation\n",
    "\n",
    "Intuitive: Approximate the posterior distribution with a multivariate normal distribution centered at the mode of posterior. \n",
    "\n",
    "**Def.3 Laplace Approximation:** Given a MAP estimation $\\theta_{MAP}=argmax_{\\theta}\\pi(\\theta|D)$, the laplace approximation of posterior on MAP is defined as:\n",
    "\\begin{align}\n",
    "& \\ln{\\pi(\\theta|D)} \\approx \\ln{\\pi(\\theta_{MAP}|D)} - \\frac{1}{2}(\\theta-\\theta_{MAP})^TA(\\theta-\\theta_{MAP}) \\\\\n",
    "& A = -\\{\\nabla_{\\theta}^2\\ln{\\pi(\\theta|D)}\\}|_{\\theta_{MAP}}\n",
    "\\end{align}\n",
    "In other words, the approximate k-dimensional multivariate normal distribution can be computed as:\n",
    "\\begin{align}\n",
    "& \\pi(\\theta|D) \\sim \\mathcal{N}_k(\\mu, \\Sigma), \\theta \\in \\mathbb{R}^k \\\\\n",
    "& \\mu=\\theta_{MAP} \\\\\n",
    "& \\Sigma=A^{-1}\n",
    "\\end{align}\n",
    "\n",
    " - Since $\\ln{\\pi(\\theta_{MAP}|D)}$ is a constant, we could skip the computation. The kernel is already sufficiently defined by the 2nd term.\n",
    " - Matrix $A$ is the minus of the posterior's Hessian at the MAP estimation. If dimension of parameter $\\theta$ is k, then the dimension of $A$ is $k \\times k$.\n",
    " - The approximate k-dimensional multivariate normal distribution has the mean of $\\theta_{MAP}$, which is basically how the method is designed, and the covariance matrix is the inverse of $A$. Considering the cost of computing inverse matrix is high($O(k^3)$), the dimension of parameters k should be limited.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98d608c",
   "metadata": {},
   "source": [
    "## Multivariate Normal Distribution\n",
    "\n",
    "Multivariate normal distribution is the generalization of normal distribution to higher dimension. Suppose the dimension is k, then we denote the k dimension multivariate normal distribuion as $\\mathcal{N}_k(\\mu, \\Sigma)$, where $\\Sigma$ is semi-positive definite. In addition, the distribution is non-degenerate, if $\\Sigma$ is positive definite. For simplicity we assume that a distribution is always non-degenerate.\n",
    "\n",
    "Properties:\n",
    " - mean: $\\mu$\n",
    " - mode: $\\mu$\n",
    " - variance: $\\Sigma$\n",
    "\n",
    "Probability density function(non-degenerate):\n",
    "\\begin{align}\n",
    "f(X) = \\frac{exp(-\\frac{1}{2}(X-\\mu)^T\\Sigma^{-1}(X-\\mu))}{(2\\pi)^{\\frac{k}{2}}det(\\Sigma)^{\\frac{1}{2}}}\n",
    "\\end{align}\n",
    "\n",
    "Apparently the point mass at is $f(X=\\mu)=(2\\pi)^{-\\frac{k}{2}}det(\\Sigma)^{-\\frac{1}{2}}$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
