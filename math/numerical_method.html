
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Numerical Method &#8212; TreeHouse</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/logo.webp"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Data Structure &amp; Algorithm" href="../comp/data_structure.html" />
    <link rel="prev" title="Stochastic Method" href="stochastic_method.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.webp" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">TreeHouse</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Statistics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../stats/language_model.html">
   Probabilistic Language Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../stats/sampling.html">
   Sampling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../stats/bayesian_inference.html">
   Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../stats/normalization.html">
   Normalization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../stats/markov-chain_mdp.html">
   Markov Chain &amp; Markov Decision Process
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Mathematics
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="multivariate_calculus.html">
   Multivariate Calculus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear_programming.html">
   Linear Programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="non_linear_unscontrained.html">
   Non-linear Programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="stochastic_method.html">
   Stochastic Method
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Numerical Method
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Computation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../comp/data_structure.html">
   Data Structure &amp; Algorithm
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../comp/search_algorithm.html">
   Search
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../comp/reinforcement_learning.html">
   Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../comp/neural_networks.html">
   Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../comp/logistic_regression.html">
   Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../comp/ensemble_learning.html">
   Ensemble Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Topic
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../topic/GCN.html">
   Fraud Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../topic/CV.html">
   Computer Vision
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/yingshuangfan/treehouse/master?urlpath=lab/tree/math/numerical_method.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/yingshuangfan/treehouse/blob/master/math/numerical_method.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/math/numerical_method.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convolutional-neural-network">
   Convolutional Neural Network
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolution-operators">
     Convolution Operators
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernel-methods">
   Kernel Methods
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aronszajna-theorem">
     Aronszajna Theorem
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#repoducing-kernel-hilbert-space-rkhs">
     Repoducing Kernel Hilbert Space(RKHS)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rkhs-vs-kernel">
     RKHS vs. Kernel
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#linear-kernel">
       Linear Kernel
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#polynomial-kernel-of-degree-2">
       Polynomial Kernel (of degree 2)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#polynomial-kernel-general-case">
       polynomial Kernel (general case)
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#combining-kernels">
     Combining Kernels
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#smoothness-of-the-rkhs">
     Smoothness of the RKHS
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary">
     Summary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kernel-tricks">
     Kernel Tricks
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#kernel-tricks-for-supervised-learning">
       Kernel Tricks for Supervised Learning
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#representer-theorem">
       Representer Theorem
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#least-square-regression-with-general-function-space">
       Least Square Regression with general function space
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kernel-ridge-regression-krr">
     Kernel Ridge Regression (KRR)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#krr-in-matrix-form">
       KRR in matrix form
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Numerical Method</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#convolutional-neural-network">
   Convolutional Neural Network
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convolution-operators">
     Convolution Operators
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernel-methods">
   Kernel Methods
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aronszajna-theorem">
     Aronszajna Theorem
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#repoducing-kernel-hilbert-space-rkhs">
     Repoducing Kernel Hilbert Space(RKHS)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rkhs-vs-kernel">
     RKHS vs. Kernel
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#linear-kernel">
       Linear Kernel
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#polynomial-kernel-of-degree-2">
       Polynomial Kernel (of degree 2)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#polynomial-kernel-general-case">
       polynomial Kernel (general case)
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#combining-kernels">
     Combining Kernels
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#smoothness-of-the-rkhs">
     Smoothness of the RKHS
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary">
     Summary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kernel-tricks">
     Kernel Tricks
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#kernel-tricks-for-supervised-learning">
       Kernel Tricks for Supervised Learning
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#representer-theorem">
       Representer Theorem
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#least-square-regression-with-general-function-space">
       Least Square Regression with general function space
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kernel-ridge-regression-krr">
     Kernel Ridge Regression (KRR)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#krr-in-matrix-form">
       KRR in matrix form
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="numerical-method">
<h1>Numerical Method<a class="headerlink" href="#numerical-method" title="Permalink to this headline">#</a></h1>
<p>Goals:</p>
<ul class="simple">
<li><p>Design computational algorithms.</p></li>
<li><p>Understand why they are efficient.</p></li>
<li><p>Discuss their limitations.</p></li>
<li><p>Design fast algorithms.</p></li>
</ul>
<p>The limitations for DNN:</p>
<ul class="simple">
<li><p>Expressibility: how to explain a DNN succeeding to approximate a certain function?</p></li>
<li><p>Learning and Generalization: how to explain why some optimization algorithms are better at generalization?</p></li>
<li><p>Robustness and Stability: why a DNN could be sensitive to input features or training data?</p></li>
</ul>
<section id="convolutional-neural-network">
<h2>Convolutional Neural Network<a class="headerlink" href="#convolutional-neural-network" title="Permalink to this headline">#</a></h2>
<section id="convolution-operators">
<h3>Convolution Operators<a class="headerlink" href="#convolution-operators" title="Permalink to this headline">#</a></h3>
<p><strong>Def.1-1 1-d Convolution for continuous functions:</strong> Given two functions <span class="math notranslate nohighlight">\(x(u)\)</span> and <span class="math notranslate nohighlight">\(h(u)\)</span>, their convolution is defined as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
c(t) = (x * h)(t) = \int_{-\infty}^{\infty} x(u)h(t-u) \,du
\end{align*}\]</div>
<p><strong>Def.1-2 1-d Convolution for discrete sequences:</strong> Given two sequences <span class="math notranslate nohighlight">\(a = (a_0,a_1,...,a_{m-1})\)</span> and <span class="math notranslate nohighlight">\(b = (b_0,b_1,...,b_{m-1})\)</span>, their convolution is defined as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
c_{k} = \sum_{i=0}^k a_i b_{k-i}
\end{align*}\]</div>
<p>Comments:</p>
<ul class="simple">
<li><p>The length of two sequences can be different, we could assume that “m” in the definition is the maximum length of two sequences, where the rest elements are zeros.</p></li>
</ul>
<p><strong>Corollary.1-2-1 Matrix Form of 1-d Convolution for discrete sequences:</strong> If <span class="math notranslate nohighlight">\(c = (c_0,c_1,...c_{m-1})\)</span>, where <span class="math notranslate nohighlight">\(c_k\)</span> is defined as in Def.1-2, then the convolution operation could be written as matrix form:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
c = Ka = 
\begin{bmatrix}
a_0 b_0\\
a_0 b_1 + a_1 b_0\\
...\\
a_0 b_{m-1} + a_1 b_{m-2} + ... + a_{m-1}b_0
\end{bmatrix} =
\begin{bmatrix}
b_0\\
b_1 &amp; b_0\\
...\\
b_{m-1} &amp; b_{m-2} &amp; ... &amp; b_0
\end{bmatrix} a
\end{align*}\]</div>
<p>Comments:</p>
<ul class="simple">
<li><p>The first function <span class="math notranslate nohighlight">\(x(u)\)</span> or sequence <span class="math notranslate nohighlight">\(a\)</span> is referred as the “input”, while the second function <span class="math notranslate nohighlight">\(h(u)\)</span> or sequence <span class="math notranslate nohighlight">\(b\)</span> is referred as the “kernel”.</p></li>
<li><p>The definition of convolution could be further generalize to higher dimension.</p></li>
</ul>
<p><strong>Def.1-3 2-d Convolution for continuous functions:</strong> SKIPPED</p>
<p><strong>Def.1-4 2-d Convolution for matrices:</strong> Given two (finite) matrices <span class="math notranslate nohighlight">\(I \in \mathcal{R}^{m_1\times n_1}\)</span> and <span class="math notranslate nohighlight">\(K \in \mathcal{R}^{m_2\times n_2}\)</span>, their convolution <span class="math notranslate nohighlight">\(S \in \mathcal{R}^{(m_1+m_2-1)\times(n_1+n_2-1)}\)</span> is defined as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
S(i,j) = (I*K)(i,j) =  \sum_{k_1=0}^{m_1-1} \sum_{k_2=0}^{n_1-1} I(k_1,k_2)K(i-k_1,j-k_2)
\end{align*}\]</div>
<p>Reference: WIKI: “Multidimensional convolution with one-dimensional convolution methods”</p>
<p><strong>Def.1-5 2-d Convolution for matrices in Cross-Correlation Form:</strong> The convolution operation defined in 1-4 where we flipped the kernel w.r.t the input is useful for writing proofs. On the other hand, the so-called “Cross-Correlation” version of operation is most implemented for DNN in practice. The main reason is that the size of kernal is much smaller compared to real-world dataset, thus the output size is also more compact by the later definition.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
S(i,j) = (I*K)(i,j) =  \sum_{k_1=0}^{m_2-1} \sum_{k_2=0}^{n_2-1} I(i+k_1,j+k_2)K(k_1,k_2)
\end{align*}\]</div>
<p>Comments: Notice that the size of output <span class="math notranslate nohighlight">\(S \in \mathcal{R}^{(m_1-m_2+1)\times(n_1-n_2+1)}\)</span> is smaller than the definition in Def.1-4. However, if we assume that the size of input is close to infinite(big dataset), then the difference could be ignored.</p>
<p><strong>Corollary.1-4-1 Matrix Form of 2-d Convolution for matrices:</strong> For <span class="math notranslate nohighlight">\(S \in \mathcal{R}^{(m_1+m_2-1)\times(n_1+n_2-1)}\)</span> as defined in Def.1-4, the convolution operation could be written as matrix form, where <span class="math notranslate nohighlight">\(T \in \mathcal{R}^{(m_1+m_2-1)\times(n_1+n_2-1)}\)</span>:</p>
<ol class="simple">
<li><p>Let <span class="math notranslate nohighlight">\(T_k \in \mathcal{R}^{(n_1+n_2-1)\times n_1}\)</span> be the k-th (sub)block of matrix <span class="math notranslate nohighlight">\(T\)</span>, where <span class="math notranslate nohighlight">\(k=1,2,...,n_1\)</span>. <span class="math notranslate nohighlight">\(T_k\)</span> is a Toeplitz matrix constructed by the k-th row of matrix <span class="math notranslate nohighlight">\(H\)</span>:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
H = 
\begin{bmatrix}
K &amp; 0\\
0 &amp; 0
\end{bmatrix}
\end{align*}\]</div>
<p>For instance, the k-th block takes the form, where <span class="math notranslate nohighlight">\(K_{ij}\)</span> denotes the element of <span class="math notranslate nohighlight">\(K[i][j]\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
T_k =
\begin{bmatrix}
K_{k0} &amp; 0 &amp; ... \\
K_{k1} &amp; K_{k0} &amp; 0 &amp; ... \\
K_{k2} &amp; K_{k1} &amp; K_{k0} &amp; 0 &amp; ... \\
\vdots &amp; \ddots &amp; \ddots \\
K_{km_1} &amp; K_{k(m_1-1)} &amp; ... \\
0 &amp; K_{km_1} &amp; K_{k(m_1-1)} &amp; ...\\
\vdots &amp; \ddots &amp; \ddots \\
\end{bmatrix}
\end{align*}\]</div>
<ol class="simple">
<li><p>Generate the complete doubly-block Toeplitz matrix <span class="math notranslate nohighlight">\(T \in \mathcal{R}^{(m_1+m_2-1)(n_1+n_2-1)\times m_1 n_1}\)</span></p></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
T = 
\begin{bmatrix}
T_1 &amp; T_2 &amp; ... &amp; T_{m1}\\
T_2 &amp; T_1 &amp; ... &amp; T_{m1-1}\\
\vdots &amp; \ddots \\
T_{m1+m2-1} &amp; ... 
\end{bmatrix}
\end{align*}\]</div>
<ol class="simple">
<li><p>Let vector <span class="math notranslate nohighlight">\(I^* \in \mathcal{R}^{m_1 n_1}\)</span> denotes the flatten input matrix <span class="math notranslate nohighlight">\(I\)</span>, and vector <span class="math notranslate nohighlight">\(S^* \in \mathcal{R}^{(m_1+m_2-1)(n_1+n_2-1)}\)</span> denotes the flatten convolutional result matrix <span class="math notranslate nohighlight">\(S\)</span>, then the convolution operation can be presented by matrix dot operation:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
S^* = T I^*
\end{align*}\]</div>
<p>Comments:</p>
<ul class="simple">
<li><p>The definition we mentioned in Def.1-2 and Def.1-4 are element-wise. However, for those indices which are invalid in the original matrix or sequence, we could assume the values are zeros.</p></li>
<li><p>Even though the result of convolution has much higher dimension by definition, it is usually very sparse and could be represented by low dimensional vector(s) instead.</p></li>
</ul>
</section>
</section>
<section id="kernel-methods">
<h2>Kernel Methods<a class="headerlink" href="#kernel-methods" title="Permalink to this headline">#</a></h2>
<p>Intuition: Develop a vasetile algorithm without making any assumptions on the training dataset. (Recall the intuition for non-parametric bayesian)</p>
<p><strong>Def.2-1 Positive Definitive(p.d.) Kernels:</strong> A positive kernel defined on set <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> should satisfied:</p>
<ul>
<li><p>symmetric: <span class="math notranslate nohighlight">\(K(x,x') = K(x,x')\)</span>.</p></li>
<li><p>positive definite: for any co-efficients <span class="math notranslate nohighlight">\((a_1,a_2,...,a_N) \in \mathcal{R}^N\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \sum_{i=1}^N \sum_{j=1}^N a_i a_j K(x^{(i)},x^{(j)}) \ge 0
    \end{align*}\]</div>
</li>
</ul>
<p><strong>Corollary.2-1-1 Proof of p.d. Kernels by PSD matrix:</strong> Let the <strong>similarity matrix K</strong> be defined as <span class="math notranslate nohighlight">\([K]_{ij} = K(x^i,x^j)\)</span>, then the kernel is positive definite if and only if its similarity matrix is <strong>positive semi-definite</strong>.</p>
<ul class="simple">
<li><p>Similarity matrix K is symmetric by definition, thus we could apply eigen-decomposition on K.</p></li>
</ul>
<p>Example: since symmetry is very easy to observe, we only illustrate the proof of non-negativity.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathcal{X} = \mathcal{R}\)</span>, <span class="math notranslate nohighlight">\(K(x,x')=xx'\)</span>. Proof:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    &amp;\sum_{i=1}^N \sum_{j=1}^N a_i a_j x^i x^j \\
    &amp;= \sum_{i=1}^N a_i x^i (\sum_{j=1}^N  a_j x^j) \\
    &amp;= (\sum_{i=1}^N a_i x^i)^2 \ge 0
    \end{align*}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{X} = \mathcal{R}^d\)</span>, <span class="math notranslate nohighlight">\(K(x,x')=\langle x,x'\rangle \)</span>. Proof:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    &amp;\sum_{i=1}^N \sum_{j=1}^N a_i a_j \langle x^i,x^j \rangle \\
    &amp;= \sum_{i=1}^N \sum_{j=1}^N \langle a_i x^i,a_j x^j \rangle \\
    &amp;= \langle \sum_{i=1}^N a_i x^i, \sum_{j=1}^N a_j x^j \rangle \\
    &amp;= \| \sum_{i=1}^N a_i x^i \|_d^2  \ge 0
    \end{align*}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{X}\)</span> ia ANY set, <span class="math notranslate nohighlight">\(\phi: \mathcal{X}^2 \to \mathcal{R}^d\)</span>, <span class="math notranslate nohighlight">\(K(x,x')=\langle \phi(x),\phi(x')\rangle\)</span>. Proof:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    &amp;\sum_{i=1}^N \sum_{j=1}^N a_i a_j \langle \phi(x^{(i)}),\phi(x^{(j)}) \rangle \\
    &amp;= \sum_{i=1}^N \sum_{j=1}^N \langle a_i\phi(x^{(i)}),a_j\phi(x^{(j)}) \rangle \\
    &amp;= \langle \sum_{i=1}^N a_i\phi(x^{(i)}),\sum_{j=1}^N a_j\phi(x^{(j)}) \rangle \\
    &amp;= \| \sum_{i=1}^N a_i\phi(x^{(i)}) \|_d^2 \ge 0
    \end{align*}\]</div>
</li>
</ul>
<section id="aronszajna-theorem">
<h3>Aronszajna Theorem<a class="headerlink" href="#aronszajna-theorem" title="Permalink to this headline">#</a></h3>
<p>Kernel K is PD if and only if we could find:</p>
<ol class="simple">
<li><p>a Hilbert Space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>.</p></li>
<li><p>a mapping <span class="math notranslate nohighlight">\(\phi: \mathcal{X} \to \mathcal{H}\)</span>.</p></li>
</ol>
<p>such that for any pair of <span class="math notranslate nohighlight">\(x,x' \in \mathcal{X}\)</span>, we have the kernel value(distance between x and x’ as defined by kernel) equals to the inner product of mapping vectors:
\begin{align*}
K(x,x’) = \langle \phi(x),\phi(x’) \rangle_\mathcal{H}
\end{align*}</p>
<p><strong>Comments:</strong></p>
<ul class="simple">
<li><p>The inner product is defined by the Hilbert Space we proposed.</p></li>
<li><p>The proof of theorem is complicated when the dimension of dataset <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> is infinite, for the finite case we could use simple linear algebra for proof. Thus we introduce the idea of RKHS to help us simplify the procedure.</p></li>
<li><p>In fact, the RKHS and kernel are uniquely determined to each other.</p></li>
</ul>
<p><strong>Proof: (for the FINITE case)</strong> Conversely proof TBC.</p>
<p>Suppose the dataset has N data points, thus the similarity matrix has dimension <span class="math notranslate nohighlight">\(N \times N\)</span> and could be diagnalized on an orthonormal(orthogonal + normal) basis of eigenvector <span class="math notranslate nohighlight">\((u_1,u_2,...u_N)\)</span>. For a p.d. kernel, we know its similarity matrix is p.s.d, thus the eigenvalues are non-genative with <span class="math notranslate nohighlight">\(\lambda_N \ge \lambda_{N-1} \cdots \ge \lambda_1 \ge 0\)</span>. Thus the <span class="math notranslate nohighlight">\(K[i,j]\)</span> element could be written as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
K(x_i,x_j) = K_{ij} = \left(\sum_{k=1}^N \lambda_k u_k u_k^T\right)_{ij} = \sum_{k=1}^N \lambda_k (u_k)_i (u_k)_j
\end{align*}\]</div>
<p>Suppose we define a mapping <span class="math notranslate nohighlight">\(\phi\)</span> such that <span class="math notranslate nohighlight">\(K(x_i,x_j) = \langle \phi(x_i),\phi(x_j) \rangle_{\mathcal{R}^N}\)</span>. By observation, let <span class="math notranslate nohighlight">\(\phi(x_i) = (\sqrt \lambda_1 (u_1)_i, \cdots, \sqrt \lambda_N (u_N)_i)\)</span>. In summary, in finite dimensional hilbert space, we could always find a function <span class="math notranslate nohighlight">\(\phi\)</span> which statisfied the aforementioned statement given any p.d. kernel(and vice versa), thus the theorem is proved.</p>
<p><strong>Proof: (for the INFINITE case, using the concept of RKHS)</strong> Conversely proof TBC.</p>
<p>If kernel function K is p.d. on dataset <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>, then it is a reproducing kernel of a hilbert space <span class="math notranslate nohighlight">\(\mathcal{H} \)</span>. Let function <span class="math notranslate nohighlight">\(\phi: \mathcal{X} \to \mathcal{H}\)</span> be the kernel map defined by <span class="math notranslate nohighlight">\(\phi(x) = K_x\)</span> at any point x. Thus by the reproducing property, we have <span class="math notranslate nohighlight">\(f(x) = \langle f,K_x \rangle_\mathcal{H}\)</span>. In other words, for any function f fixed at another point y, we have <span class="math notranslate nohighlight">\(K(x,y) = \langle \phi(x),\phi(y) \rangle_\mathcal{H}\)</span>, thus the theorem is proved.</p>
</section>
<section id="repoducing-kernel-hilbert-space-rkhs">
<h3>Repoducing Kernel Hilbert Space(RKHS)<a class="headerlink" href="#repoducing-kernel-hilbert-space-rkhs" title="Permalink to this headline">#</a></h3>
<p><strong>Def.2-2 Reproducing Kernel:</strong> The function <span class="math notranslate nohighlight">\(K: \mathcal{X}^2 \to \mathcal{R}\)</span> is called a repoducing kernel of hilbert space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> if it satisfies:</p>
<ol>
<li><p>If we fixed point <span class="math notranslate nohighlight">\(x\)</span> for kernel function <span class="math notranslate nohighlight">\(K(x,x')\)</span>, then all functions <span class="math notranslate nohighlight">\(K_x: \mathcal{X} \to \mathcal{R}\)</span> are contained in <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>.</p></li>
<li><p>For every point <span class="math notranslate nohighlight">\(x\)</span> and every function <span class="math notranslate nohighlight">\(f\)</span> in <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>, the <strong>repoducing property</strong> holds:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    f(x) = \langle f,K_x \rangle_\mathcal{H}
    \end{align*}\]</div>
</li>
</ol>
<p>Comments:</p>
<ul class="simple">
<li><p>RKHS is a special subset where the repoducing property holds. This property is crucial for the proof of Aronszajna Theorem.</p></li>
<li><p>The kernel is called “reproducing”, because it could repoduce the value of any function f at point x with the inner-product between f and kernel function fixed at point x.</p></li>
<li><p>The mapping function <span class="math notranslate nohighlight">\(\phi(x) = K_x\)</span> maps the finite dataset <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> to a high-dimensional and infinite hilbert space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>. We call the hilbert space is infinite because the number of functions in the space is unlimited.</p></li>
</ul>
<p><strong>Corollary.2-2-1 Uniqueness of repoducing kernel and RKHS:</strong> There are <strong>uniquely defined</strong>! If <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is a RKHS, then it has a unique repoducing kernel. Conversely, if a function K could be the repoducing kernel of at most one RKHS.</p>
<p>Proof: TBC.</p>
<p><strong>Corollary.2-2-2 p.d. Kernel = reproducing Kernel:</strong> A function K is positive definite if and only if it is a reproducing kernel(of a RKHS).</p>
<p>Proof: TBC.</p>
<p><strong>Def.2-3 Alternative definition of RKHS:</strong> A hilbert space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is a RKHS if and only if, for any point x in dataset <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>, the linear mapping F as defined is continuous:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
F: &amp; \mathcal{H} \to \mathcal{R} \\
&amp; f \to f(x)
\end{align*}\]</div>
<p><strong>Corollary.2-3-1 Pointwise Convergence of RKHS:</strong> The convergence of RKHS is always pointwise convergence. If a series of function <span class="math notranslate nohighlight">\(\{f_n\}\)</span> converges to function <span class="math notranslate nohighlight">\(f\)</span>, then we could assume that at any point x, we have series <span class="math notranslate nohighlight">\(\{f_n(x)\}\)</span> converges to value <span class="math notranslate nohighlight">\(f(x)\)</span>.</p>
<p><strong>Proof:</strong> TBC.</p>
<p><strong>Property:</strong> Uniqueness of repoducing kernel and RKHS.</p>
</section>
<section id="rkhs-vs-kernel">
<h3>RKHS vs. Kernel<a class="headerlink" href="#rkhs-vs-kernel" title="Permalink to this headline">#</a></h3>
<p>Intuition: If a certain kernel function is proposed, how to find the RKHS for such kernel? Supposed that the kernel is positive definite. The general procedure of finding a RKHS is:</p>
<ol class="simple">
<li><p>Look for an definition of inner-product.</p></li>
<li><p>Propose a candidate RKHS <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>(with the inner-product).</p></li>
<li><p>Check if space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is a hilbert space.</p></li>
<li><p>Check if space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is a RKHS.</p></li>
</ol>
<section id="linear-kernel">
<h4>Linear Kernel<a class="headerlink" href="#linear-kernel" title="Permalink to this headline">#</a></h4>
<p>Task: find a RKHS for the defined kernel: <span class="math notranslate nohighlight">\(K(x,x')=\langle x,x' \rangle_{\mathcal{R}^d}\)</span>, where the dataset is defined in <span class="math notranslate nohighlight">\(\mathcal{R}^d\)</span>.</p>
<p>TBC.</p>
</section>
<section id="polynomial-kernel-of-degree-2">
<h4>Polynomial Kernel (of degree 2)<a class="headerlink" href="#polynomial-kernel-of-degree-2" title="Permalink to this headline">#</a></h4>
<p>Task: find a RKHS for the defined kernel: <span class="math notranslate nohighlight">\(K(x,x')=\langle x,x' \rangle_{\mathcal{R}^d}^2\)</span>, where the dataset is defined in <span class="math notranslate nohighlight">\(\mathcal{R}^d\)</span>.</p>
<p>TBC.</p>
</section>
<section id="polynomial-kernel-general-case">
<h4>polynomial Kernel (general case)<a class="headerlink" href="#polynomial-kernel-general-case" title="Permalink to this headline">#</a></h4>
<p>Task: find a RKHS for the defined kernel: <span class="math notranslate nohighlight">\(K(x,x')=\langle x,x' \rangle_{\mathcal{R}^d}^n\)</span>, where the dataset is defined in <span class="math notranslate nohighlight">\(\mathcal{R}^d\)</span>.</p>
<p>TBC.</p>
</section>
</section>
<section id="combining-kernels">
<h3>Combining Kernels<a class="headerlink" href="#combining-kernels" title="Permalink to this headline">#</a></h3>
<p>Intuition: Some form of combination on kernels could still hold the property of positive definite. Thus we could take the advantage and skip the long proof from scratch.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(K_1 + K_2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(K_1 K_2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(c K_1\)</span>, where c is constant.</p></li>
<li><p>K(x,x’) = \lim_{n \to \infty} K_i(x,x’), where {K_i} is a sequence of p.d. kernels that converges pointwisely to a function K.</p></li>
</ul>
</section>
<section id="smoothness-of-the-rkhs">
<h3>Smoothness of the RKHS<a class="headerlink" href="#smoothness-of-the-rkhs" title="Permalink to this headline">#</a></h3>
<p><strong>Def.2-4 Smoothness:</strong> The norm of a function in the RKHS controlls fast the function varies over the dataset <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> with respect to the geometry defined by the kernel function.</p>
<p><strong>Comments:</strong>:</p>
<ul class="simple">
<li><p>A smooth RKHS has small norm for any given function, which indicates that any difference of input would not effect the output significantly. Thus the property of smoothness is a wanted feature for machine learning.</p></li>
</ul>
</section>
<section id="summary">
<h3>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>A p.d. kernel can be regarded as inner-product in some hilbert space after embedding(from data space <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> to hilert space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>).</p></li>
<li><p>A realization of this embedding is the RKHS, which is not restricted by the dataset nor the kernel function.</p></li>
<li><p>The RKHS is a function space defined on <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>.</p></li>
<li><p>The norm of function in RKHS is related to its degree of smoothness, with respect to the metric defined by the kernel function.</p></li>
</ul>
</section>
<section id="kernel-tricks">
<h3>Kernel Tricks<a class="headerlink" href="#kernel-tricks" title="Permalink to this headline">#</a></h3>
<p>Intuition: The embedding from original data space <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> to a higher dimensional hilbert space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> could be beneficial for a machine learning algorithm. For instance, for a linear regression classifier on a linear unseperable dataset, if the kernel tricks could map the dataset into a higher dimensional space where the dataset is linear separable, then the performance of such classifier is enhanced significantly.</p>
<p><strong>Basic paradigm of kernel tricks:</strong></p>
<ol class="simple">
<li><p>Choose a mapping function <span class="math notranslate nohighlight">\(\phi: \mathcal{X} \to \mathcal{H}\)</span>. Noted that the target hilbert space could has infinite dimension.</p></li>
<li><p>Embbed the original labeled dataset <span class="math notranslate nohighlight">\(S\{x_i, y_i\}\)</span> into <span class="math notranslate nohighlight">\(S^*\{\phi(x_i),y_i\}\)</span>, where <span class="math notranslate nohighlight">\(y\)</span> denotes the label.</p></li>
<li><p>Train a linear predictor over <span class="math notranslate nohighlight">\(S^*\)</span>.</p></li>
<li><p>Given the test point x_{new}, predict the label with predictor: <span class="math notranslate nohighlight">\(S^*(\phi(x_{new}))\)</span>. ???</p></li>
</ol>
<section id="kernel-tricks-for-supervised-learning">
<h4>Kernel Tricks for Supervised Learning<a class="headerlink" href="#kernel-tricks-for-supervised-learning" title="Permalink to this headline">#</a></h4>
<p>Given the space of input <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and space of output <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>, and the training dataset <span class="math notranslate nohighlight">\(\{(x_i, y_i)\}_{i=1}^N\)</span>, the goal is to learn a predictor f such that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\min_f \frac{1}{N}\sum_{i=1}^N loss(f(x_i), y_i) + \lambda\Omega(\|f\|_{\mathcal{H}}) 
\end{align*}\]</div>
<p>For instance, if we choose MSE as loss function, l2 norm as norm function, linear model as predictor, then the algorithm is the logistric regression.</p>
<p>The kernel tricks is mainly focus on the range for optimization search, or in other words, the function space for f. Since in the kernel methods, we learn the target function f from a RKHS <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>, we start from <span class="math notranslate nohighlight">\(f \in \mathcal{H}\)</span>. However, the function space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is infinite, in practice we need to narrow down the search space by the representer theorem.</p>
</section>
<section id="representer-theorem">
<h4>Representer Theorem<a class="headerlink" href="#representer-theorem" title="Permalink to this headline">#</a></h4>
<p>Let <span class="math notranslate nohighlight">\(\Phi: \mathcal{R}^{N+1} \to \mathcal{R}\)</span> be a stricly increasing function, then the optimization problem:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
min_{f \in \mathcal{H}} \Phi\left(f(x_1),\cdots,f(x_N),\|f\|_{\mathcal{H}}\right)
\end{align*}\]</div>
<p>has the repsentation form of:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
f(x)=\sum_{i=1}^N \alpha_i K(x_i,x) = \sum_{i=1}^N \alpha_i K_{x_i}(x)
\end{align*}\]</div>
<p>In other words, the result function f lies in a subset of RKHS: <span class="math notranslate nohighlight">\(span\{K_{x_1}(x),\cdots,K_{x_N}(x)\}\)</span>. Notice that the subset is finite with the dimension of N. Therefore, in practice we only need to search the finite subset for the optimal solution.</p>
<p><strong>Proof:</strong> TBC.</p>
</section>
<section id="least-square-regression-with-general-function-space">
<h4>Least Square Regression with general function space<a class="headerlink" href="#least-square-regression-with-general-function-space" title="Permalink to this headline">#</a></h4>
</section>
</section>
<section id="kernel-ridge-regression-krr">
<h3>Kernel Ridge Regression (KRR)<a class="headerlink" href="#kernel-ridge-regression-krr" title="Permalink to this headline">#</a></h3>
<p>Given a kernel function K, train the coefficient vector <span class="math notranslate nohighlight">\(\alpha \in \mathcal{R}^N\)</span> by solving the optimization problem:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\min_f \frac{1}{N}\sum_{i=1}^N loss(f(x_i),y_i) + \lambda \|f\|_{\mathcal{H}}^2 
\end{align*}\]</div>
<p>By the representer theorem, any solution should takes the form of <span class="math notranslate nohighlight">\(f^*=\sum_{i=1}^N \alpha_i K_{x_i}(x)\)</span>.</p>
<section id="krr-in-matrix-form">
<h4>KRR in matrix form<a class="headerlink" href="#krr-in-matrix-form" title="Permalink to this headline">#</a></h4>
<p>Suppose K is the similarity matrix of kernel function, such that <span class="math notranslate nohighlight">\(K_{ij}=K(x_i,x_j)\)</span>.</p>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./math"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="stochastic_method.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Stochastic Method</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../comp/data_structure.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Data Structure &amp; Algorithm</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Yingshuang Fan<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>