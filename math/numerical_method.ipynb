{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "813913db",
   "metadata": {},
   "source": [
    "# Numerical Method\n",
    "\n",
    "Goals:\n",
    " - Design computational algorithms.\n",
    " - Understand why they are efficient.\n",
    " - Discuss their limitations.\n",
    " - Design fast algorithms.\n",
    " \n",
    "The limitations for DNN:\n",
    " - Expressibility: how to explain a DNN succeeding to approximate a certain function?\n",
    " - Learning and Generalization: how to explain why some optimization algorithms are better at generalization?\n",
    " - Robustness and Stability: why a DNN could be sensitive to input features or training data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd26795",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "### Convolution Operators\n",
    "\n",
    "**Def.1-1 1-d Convolution for continuous functions:** Given two functions $x(u)$ and $h(u)$, their convolution is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "c(t) = (x * h)(t) = \\int_{-\\infty}^{\\infty} x(u)h(t-u) \\,du\n",
    "\\end{align*}\n",
    "\n",
    "**Def.1-2 1-d Convolution for discrete sequences:** Given two sequences $a = (a_0,a_1,...,a_{m-1})$ and $b = (b_0,b_1,...,b_{m-1})$, their convolution is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "c_{k} = \\sum_{i=0}^k a_i b_{k-i}\n",
    "\\end{align*}\n",
    "\n",
    "**Corollary.1-2-1 Matrix Form of 1-d Convolution for discrete sequences:** If $c = (c_0,c_1,...c_{m-1})$, where $c_k$ is defined as in Def.1-2, then the convolution operation could be written as matrix form:\n",
    "\n",
    "\\begin{align*}\n",
    "c = Ka = \n",
    "\\begin{bmatrix}\n",
    "a_0 b_0\\\\\n",
    "a_0 b_1 + a_1 b_0\\\\\n",
    "...\\\\\n",
    "a_0 b_{m-1} + a_1 b_{m-2} + ... + a_{m-1}b_0\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "b_0\\\\\n",
    "b_1 & b_0\\\\\n",
    "...\\\\\n",
    "b_{m-1} & b_{m-2} & ... & b_0\n",
    "\\end{bmatrix} a\n",
    "\\end{align*}\n",
    "\n",
    "Comments:\n",
    " - The first function $x(u)$ or sequence $a$ is referred as the \"input\", while the second function $h(u)$ or sequence $b$ is referred as the \"kernel\".\n",
    " - The definition of convolution could be further generalize to higher dimension.\n",
    " \n",
    "\n",
    " \n",
    " \n",
    "**Def.1-3 2-d Convolution for continuous functions:** SKIPPED\n",
    "\n",
    "**Def.1-4 2-d Convolution for matrices:** Given two (finite) matrices $I \\in \\mathcal{R}^{m_1\\times n_1}$ and $K \\in \\mathcal{R}^{m_2\\times n_2}$, their convolution $S \\in \\mathcal{R}^{(m_1+m_2-1)\\times(n_1+n_2-1)}$ is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "S(i,j) = (I*K)(i,j) =  \\sum_{k_1=0}^{m_1-1} \\sum_{k_2=0}^{n_1-1} I(k_1,k_2)K(i-k_1,j-k_2)\n",
    "\\end{align*}\n",
    "\n",
    "Reference: WIKI: \"Multidimensional convolution with one-dimensional convolution methods\"\n",
    "\n",
    "\n",
    "**Corollary.1-4-1 Matrix Form of 2-d Convolution for matrices:** For $S \\in \\mathcal{R}^{(m_1+m_2-1)\\times(n_1+n_2-1)}$ as defined in Def.1-4, the convolution operation could be written as matrix form, where $T \\in \\mathcal{R}^{(m_1+m_2-1)\\times(n_1+n_2-1)}$:\n",
    "\n",
    "1. Let $T_k \\in \\mathcal{R}^{(n_1+n_2-1)\\times n_1}$ be the k-th (sub)block of matrix $T$, where $k=1,2,...,n_1$. $T_k$ is a Toeplitz matrix constructed by the k-th row of matrix $H$:\n",
    "\n",
    "\\begin{align*}\n",
    "H = \n",
    "\\begin{bmatrix}\n",
    "K & 0\\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "For instance, the k-th block takes the form, where $K_{ij}$ denotes the element of $K[i][j]$:\n",
    "\\begin{align*}\n",
    "T_k =\n",
    "\\begin{bmatrix}\n",
    "K_{k0} & 0 & ... \\\\\n",
    "K_{k1} & K_{k0} & 0 & ... \\\\\n",
    "K_{k2} & K_{k1} & K_{k0} & 0 & ... \\\\\n",
    "\\vdots & \\ddots & \\ddots \\\\\n",
    "K_{km_1} & K_{k(m_1-1)} & ... \\\\\n",
    "0 & K_{km_1} & K_{k(m_1-1)} & ...\\\\\n",
    "\\vdots & \\ddots & \\ddots \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "2. Generate the complete doubly-block Toeplitz matrix $T \\in \\mathcal{R}^{(m_1+m_2-1)(n_1+n_2-1)\\times m_1 n_1}$\n",
    "\n",
    "\\begin{align*}\n",
    "T = \n",
    "\\begin{bmatrix}\n",
    "T_1 & T_2 & ... & T_{m1}\\\\\n",
    "T_2 & T_1 & ... & T_{m1-1}\\\\\n",
    "\\vdots & \\ddots \\\\\n",
    "T_{m1+m2-1} & ... \n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "3. Let vector $I^* \\in \\mathcal{R}^{m_1 n_1}$ denotes the flatten input matrix $I$, and vector $S^* \\in \\mathcal{R}^{(m_1+m_2-1)(n_1+n_2-1)}$ denotes the flatten convolutional result matrix $S$, then the convolution operation can be presented by matrix dot operation:\n",
    "\\begin{align*}\n",
    "S^* = T I^*\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Comments:\n",
    " - The definition we mentioned in Def.1-2 and Def.1-4 are element-wise. However, for those indices which are invalid in the original matrix or sequence, we could assume the values are zeros.\n",
    " - Even though the result of convolution has much higher dimension by definition, it is usually very sparse and could be represented by low dimensional vector(s) instead. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ffa609",
   "metadata": {},
   "source": [
    "### Kernel Methods\n",
    "\n",
    "**Def.2-1 Positive Definitive(PD) Kernels:** A positive kernel defined on set $\\mathcal{X}$ should satisfied: \n",
    " - symmetric: $K(x,x') = K(x,x')$.\n",
    " - positive definite: for any co-efficients $(a_1,a_2,...,a_N) \\in \\mathcal{R}^N$, \n",
    "    \\begin{align*}\n",
    "    \\sum_{i=1}^N \\sum_{j=1}^N a_i a_j K(x^{(i)},x^{(j)}) \\ge 0\n",
    "    \\end{align*}\n",
    "    \n",
    "Example:\n",
    " - $\\mathcal{X} = \\mathcal{R}$, $K(x,x')=xx'$. Proof:\n",
    "    \\begin{align*}\n",
    "    \\sum_{i=1}^N \\sum_{j=1}^N a_i a_j x^{(i)}x^{(j)} = \\sum_{i=1}^N a_i x^{(i)} (\\sum_{j=1}^N  a_j x^{(j)})= (\\sum_{i=1}^N a_i x^{(i)})^2 \\ge 0\n",
    "    \\end{align*}\n",
    " - $\\mathcal{X} = \\mathcal{R}^d$, $K(x,x')=\\langle x,x'\\rangle $. Proof:\n",
    "    \\begin{align*}\n",
    "    \\sum_{i=1}^N \\sum_{j=1}^N a_i a_j \\langle x^{(i)},x^{(j)} \\rangle = \\sum_{i=1}^N \\sum_{j=1}^N a_i a_j \\sum_{k=1}^d x_k^{(i)}x_k^{(j)} = \\sum_{k=1}^d \\sum_{i=1}^N a_i x_k^{(i)}(\\sum_{j=1}^N a_j x_k^{(j)}) = \\sum_{k=1}^d (\\sum_{i=1}^N a_i x_k^{(i)})^2 \\ge 0\n",
    "    \\end{align*}\n",
    " - $\\mathcal{X}$ ia ANY set, $\\phi: \\mathcal{X}^2 \\to \\mathcal{R}^d$, $K(x,x')=\\langle \\phi(x),\\phi(x')\\rangle$. Proof:\n",
    "    \\begin{align*}\n",
    "    \\sum_{i=1}^N \\sum_{j=1}^N a_i a_j \\langle \\phi(x^{(i)}),\\phi(x^{(j)}) \\rangle = \\sum_{i=1}^N \\sum_{j=1}^N a_i a_j \\sum_{k=1}^d \\phi(x^{(i)})_k \\phi(x^{(j)})_k = \\sum_{k=1}^d \\sum_{i=1}^N a_i \\phi(x^{(i)})_k(\\sum_{j=1}^N a_j \\phi(x^{(j)})_k )= \\sum_{k=1}^d (\\sum_{i=1}^N a_i \\phi(x^{(i)})_k)^2 \\ge 0\n",
    "    \\end{align*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
