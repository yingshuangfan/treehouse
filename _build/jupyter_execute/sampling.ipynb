{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e4d5bdd",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "\n",
    "## Direct Sampling\n",
    "\n",
    "## Likelihood(weighted) Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d0d1f6",
   "metadata": {},
   "source": [
    "## Rejection Sampling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae10114",
   "metadata": {},
   "source": [
    "## Importance Sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b6d91b",
   "metadata": {},
   "source": [
    "## Markov Chain Monte Carlo(MCMC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0c78ba",
   "metadata": {},
   "source": [
    "## Gibbs Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3b2dfc",
   "metadata": {},
   "source": [
    "## Monte Carlo Estimation (Application of Sampling)\n",
    "\n",
    "Intuition: Estimatation based on random sampling, a.k.a Monte Carlo Method. The methodology includes various implementations, a common adoption in statistics is estimating the integration(e.g. marginal probability) with complicated closed-form expression.\n",
    "\n",
    "**Def. Monte-Carlo Estimation:** The approximate expectation of $f(X)$, where random variable X follows distribution $P$, is the average of i.i.d sampled data $z_1,z_2,...,z_N$. \n",
    "\n",
    "\\begin{align*}\n",
    "E_{x \\sim p(x)}[f(x)] = \\int p(x)f(x) \\,dx \\approx \\frac{1}{N} \\sum_{i=1}^N f(z_i)\n",
    "\\end{align*}\n",
    "\n",
    "**Corollary. Estimation of Posterior Predictive with Monte Carlo Method:** The posterior predictive (integration of posterior over possible parameters $\\theta$) can be estimated by:\n",
    "\n",
    "\\begin{align*}\n",
    "P(x_{n+1}|D) &=\\int_{\\theta}{P(x_{n+1}|\\theta)P(\\theta|D)\\,d\\theta} \\\\\n",
    "& \\approx \\frac{1}{N} \\sum_{i=1}^N P(x_{n+1}|\\theta_i) \\\\\n",
    "& \\theta_i \\sim P(\\theta|D)\n",
    "\\end{align*}\n",
    "\n",
    "The estimation can be described as: \n",
    "1. Draw a sampled parameter $\\theta_i$ from the posterior $P(\\theta|D)$ with a sampler(e.g. Gibbs Sampler). \n",
    "2. Given the $\\theta_i$ in last step, draw a sampled new data $z_i$ from the likelihood $P(x_{n+1}|\\theta_i)$.\n",
    "3. Repeat step 1&2 in order to generate the i.i.d sampled data $z_1,z_2,...,z_N$, giving the size of such dataset(N) is sufficiently large, then the average of sampled data is an Monte Carlo Estimation of new data point $x_{new}$.\n",
    "\n",
    "Comments: \n",
    " - Notice that we only sample exactly ONE $z_i$ from any given $\\theta_i$ sampled from the posterior. The point here is that the size of $z_i$ doesn't matter, as long as we have sufficent samples of $\\theta_i$. In other words, the estimation here relies on taking the \"average\" over $\\,d\\theta$. For instance, a sampled data of 10k with 5k $\\theta$ is much more reliable than a sampled data of 100k with 500 $\\theta$.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}