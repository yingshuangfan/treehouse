{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e4d5bdd",
   "metadata": {},
   "source": [
    "# Bayesian Inference\n",
    "\n",
    "## Posterior\n",
    "\n",
    "**Def.1 Posterior Distribution:** The posterior distribution of parameter $\\theta$, based on observations $D=\\{x_1,...,x_n\\}$, likelihood function $L(\\theta|D)=P(D|\\theta)=\\prod_{i=1}^n{P(x_i|\\theta)}$(assuming data samples are i.i.d), prior distribution $\\pi(\\theta)$, is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\pi(\\theta|D)=\\frac{P(D|\\theta)P(\\theta)}{P(D)}=\\frac{P(D|\\theta)\\pi(\\theta)}{\\int_{\\theta}{P(D|\\theta)\\pi(\\theta)}}\n",
    "\\end{align}\n",
    "\n",
    " - To obtain the exact distribution(not the exact pdf) for $P(\\theta|D)$, we could ignore the marginal probability $P(D)$ as constant(regarding $\\theta$). Thus $P(\\theta|D) \\propto P(D|\\theta)P(\\theta)$. However, this case holds only if the form of such posterior resembles some well-known distributions, where we could find the kernel without considering the value of $P(D)$.\n",
    " \n",
    "**Corollary.1-1 Joint Posterior:** The posterior in Def.1 can be applied to multiple parameters $\\theta_1,...\\theta_m$, if given the joint prior:\n",
    "\n",
    "\\begin{align}\n",
    "\\pi(\\theta_1,...\\theta_m|D)=\\frac{P(D|\\theta_1,...\\theta_m)\\pi(\\theta_1,...\\theta_m)}{P(D)}\n",
    "\\end{align}\n",
    "\n",
    "**Corollary.1-2 Full Conditional Posterior:** For multiple parameters $\\theta_1,...\\theta_m$, the conditional posterior for i-th parameter $\\theta_i$, if given values $\\theta_{-i}$ for other parameters and data samples, is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\pi(\\theta_i|D,\\theta_{-i})&=P(\\theta_i|D,\\theta_1,...,\\theta_{i-1},\\theta_{i+1},...\\theta_m) \\\\\n",
    "&=\\frac{\\pi(\\theta_1,...\\theta_m|D)}{P(\\theta_1,...,\\theta_{i-1},\\theta_{i+1},...\\theta_m|D)} \\\\\n",
    "&=\\frac{\\pi(\\theta_1,...\\theta_m|D)}{\\int_{\\theta_i}{\\pi(\\theta_1,...\\theta_m|D)\\,d\\theta_i}}\n",
    "\\end{align}\n",
    "\n",
    " - The full conditional posterior relies on computing the joint posterior first.\n",
    " - In common practice, it is extremely complicated to compute the integral for each parameter $\\theta_i$. One possible approach is to **derive the full conditional posteroir directly from the joint posterior**, by ignoring the other parameters as constants. Thus $P(\\theta_i|D,\\theta_{-i}) \\propto P(\\theta_1,...\\theta_m|D)|_{\\theta_{-i}}$ However, this case holds only for well-known distributions.\n",
    " - The concept of full conditional is very important for Gibbs sampling method.\n",
    " \n",
    "**Corollary.1-3 Marginal Posterior:** For multiple parameters $\\theta_1,...\\theta_m$, the marginal posterior for i-th parameter $\\theta_i$, if given values $\\theta_{-i}$ for other parameters and data samples, is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\pi(\\theta_i|D)&=\\int_{\\theta_{-i}}{\\pi(\\theta_1,...\\theta_m|D)\\,d\\theta_{-i}} \\\\\n",
    "&=\\int_{\\theta_{-i}}{\\pi(\\theta_i,\\theta_{-i}|D)\\,d\\theta_{-i}} \\\\\n",
    "&=\\int_{\\theta_{-i}}{\\pi(\\theta_i|D,\\theta_{-i})P(\\theta_{-i}|D)\\,d\\theta_{-i}} \\\\\n",
    "\\end{align}\n",
    "\n",
    " - The marginal posterior relies on computing the joint posterior first, or the full conditional posterior first.\n",
    " - To obtain the exact distribution of marginal posterior, the integration might be simplified by ignoring some constants. Be aware that all parameters other than $\\theta_i$ should be integrated out. However, the case holds only for well-known distributions.\n",
    "\n",
    "## Posterior Predictive\n",
    "\n",
    "**Def.2 Posterior Predictive Distribution:** The posterior predictive distribution of parameter $\\theta$ is defined as(based on conditions given in Def.1):\n",
    "\n",
    "\\begin{align}\n",
    "P(x_{n+1}|D)=\\int_{\\theta}{P(x_{n+1},\\theta|D)\\,d\\theta}=\\int_{\\theta}{P(x_{n+1}|D,\\theta)P(\\theta|D)\\,d\\theta}=\\int_{\\theta}{P(x_{n+1}|\\theta)P(\\theta|D)\\,d\\theta}\n",
    "\\end{align}\n",
    "\n",
    " - The posterior preditive relies on computing the posterior first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a546a1",
   "metadata": {},
   "source": [
    "## Bayesian Prospective of MLP\n",
    "\n",
    "In general, a machine learning problem could also be viewed from the bayesian inference prospective. The relationship could be described as:\n",
    "\n",
    "| Bayesian Inference      | Machine Learning |\n",
    "| :-----------: | :-----------: |\n",
    "| Likelihood Function      | Loss Function       |\n",
    "| Prior Distribution   | Regularization        |\n",
    "| Posterior Distribution   | Regularized Loss Function       |\n",
    "| Maximize a Posterior Estimation(MAP)   | Minimize Regularized Loss Function       |\n",
    "| Maximize Likelihood Estimation(MLE)   | Minimize Loss Function       |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
